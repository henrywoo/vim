# ------------------------------------------------------------------------------------
# Enhancing Transformers
# Copyright (c) 2022 Thuan H. Nguyen. All Rights Reserved.
# Licensed under the MIT License [see LICENSE for details]
# ------------------------------------------------------------------------------------
# Modified from Taming Transformers (https://github.com/CompVis/taming-transformers)
# Copyright (c) 2020 Patrick Esser and Robin Rombach and BjÃ¶rn Ommer. All Rights Reserved.
# ------------------------------------------------------------------------------------

from typing import Optional, Tuple, Dict, Union, Any
from omegaconf import OmegaConf

import torch
import torch.nn as nn
from torch.optim import lr_scheduler
import torch.nn.functional as F
import pytorch_lightning as pl

from .layers import *
from ...utils.general import initialize_from_config


class CondTransformer(pl.LightningModule):
    def __init__(
        self,
        cond_key: str,
        cond: OmegaConf,
        stage1: OmegaConf,
        transformer: OmegaConf,
        path: Optional[str] = None,
        ignore_keys: List[str] = list(),
        code_shape: List[int] = None,
        scheduler: Optional[OmegaConf] = None,
    ) -> None:
        super().__init__()
        self.automatic_optimization = False

        # get condition key, code shape and scheduler
        self.cond_key = cond_key
        self.code_shape = code_shape
        self.scheduler = scheduler

        # load condition model
        self.cond_model = initialize_from_config(cond)

        # load stage1 model
        self.stage1_model = initialize_from_config(stage1)

        # load transformer
        self.transformer = initialize_from_config(transformer)

        # make the parameters in stage1 model not trainable
        self.stage1_model.eval()
        for p in self.stage1_model.parameters():
            p.requires_grad = False

        # make the parameters in condition model not trainable
        self.cond_model.eval()
        for p in self.cond_model.parameters():
            p.requires_grad = False

        if path is not None:
            self.init_from_ckpt(path, ignore_keys)

    def forward(
        self, codes: torch.LongTensor, conds: torch.LongTensor
    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:
        conds = conds.view(conds.shape[0], -1)
        logits = self.transformer(codes, conds)

        codes = codes.view(-1, codes.shape[-1])

        return logits, codes

    def init_from_ckpt(self, path: str, ignore_keys: List[str] = list()):
        sd = torch.load(path, map_location="cpu")["state_dict"]
        keys = list(sd.keys())
        for k in keys:
            for ik in ignore_keys:
                if k.startswith(ik):
                    print("Deleting key {} from state_dict.".format(k))
                    del sd[k]
        self.load_state_dict(sd, strict=False)
        print(f"Restored from {path}")

    @torch.no_grad()
    def sample(
        self,
        conds: torch.LongTensor,
        top_k: Optional[float] = None,
        top_p: Optional[float] = None,
        softmax_temperature: float = 1.0,
        use_fp16: bool = True,
    ) -> torch.FloatTensor:
        conds = conds.view(conds.shape[0], -1)
        logits, codes = self.transformer.sample(
            conds=conds,
            top_k=top_k,
            top_p=top_p,
            softmax_temperature=softmax_temperature,
            use_fp16=use_fp16,
        )

        if self.code_shape is not None:
            codes = codes.view(codes.shape[0], *self.code_shape)
        pixels = self.stage1_model.decode_codes(codes).clamp(0, 1)

        return pixels

    def get_input(self, batch: Tuple[Any, Any], key: str) -> torch.FloatTensor:
        x = batch[key]

        if len(x.shape) == 3:
            x = x[..., None]
        if x.dtype == torch.double:
            x = x.float()

        return x.contiguous()

    def shared_step(self, batch: Tuple[Any, Any], batch_idx: int) -> torch.FloatTensor:
        images = self.get_input(batch, self.stage1_model.image_key)
        conds = self.get_input(batch, self.cond_key)

        with torch.no_grad():
            codes = self.stage1_model.encode_codes(images).detach()
            conds = self.cond_model.encode_codes(conds).detach()

        logits, codes = self(codes, conds)
        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), codes.view(-1))

        return loss

    def training_step(
        self, batch: Tuple[Any, Any], batch_idx: int
    ) -> torch.FloatTensor:
        loss = self.shared_step(batch, batch_idx)
        self.log(
            "train/total_loss",
            loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        return loss

    def validation_step(
        self, batch: Tuple[Any, Any], batch_idx: int
    ) -> torch.FloatTensor:
        loss = self.shared_step(batch, batch_idx)
        self.log(
            "val/total_loss",
            loss,
            on_step=True,
            on_epoch=True,
            prog_bar=True,
            logger=True,
        )

        return loss

    def configure_optimizers(self) -> torch.optim.Optimizer:
        """
        Following minGPT:
        This long function is unfortunately doing something very simple and is being very defensive:
        We are separating out all parameters of the model into two buckets: those that will experience
        weight decay for regularization and those that won't (biases, and layernorm/embedding weights).
        We are then returning the PyTorch optimizer object.
        """
        # separate out all parameters to those that will and won't experience regularizing weight decay
        decay = set()
        no_decay = set()
        whitelist_weight_modules = (torch.nn.Linear,)
        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
        for mn, m in self.transformer.named_modules():
            for pn, p in m.named_parameters():
                fpn = "%s.%s" % (mn, pn) if mn else pn  # full param name

                if pn.endswith("bias"):
                    # all biases will not be decayed
                    no_decay.add(fpn)
                elif pn.endswith("weight") and isinstance(m, whitelist_weight_modules):
                    # weights of whitelist modules will be weight decayed
                    decay.add(fpn)
                elif pn.endswith("weight") and isinstance(m, blacklist_weight_modules):
                    # weights of blacklist modules will NOT be weight decayed
                    no_decay.add(fpn)
                elif "time_" in pn:  # for RWKV
                    no_decay.add(fpn)

        # special case the position embedding parameter in the root GPT module as not decayed
        no_decay.add("pos_emb_cond")
        no_decay.add("pos_emb_code")

        if hasattr(self.transformer, "pos_emb_depth"):
            no_decay.add("pos_emb_depth")

        # validate that we considered every parameter
        param_dict = {pn: p for pn, p in self.transformer.named_parameters()}
        inter_params = decay & no_decay
        union_params = decay | no_decay
        assert (
            len(inter_params) == 0
        ), "parameters %s made it into both decay/no_decay sets!" % (str(inter_params),)
        assert len(param_dict.keys() - union_params) == 0, (
            "parameters %s were not separated into either decay/no_decay/ignored set!"
            % (str(param_dict.keys() - union_params),)
        )

        # create the pytorch optimizer object
        optim_groups = [
            {
                "params": [param_dict[pn] for pn in sorted(list(decay))],
                "weight_decay": 0.01,
            },
            {
                "params": [param_dict[pn] for pn in sorted(list(no_decay))],
                "weight_decay": 0.0,
            },
        ]
        optimizer = [
            torch.optim.Adam(optim_groups, lr=self.learning_rate, betas=(0.9, 0.96))
        ]
        scheduler = []

        if self.scheduler is not None:
            self.scheduler.params.start = lr
            scheduler = initialize_from_config(self.scheduler)

            scheduler = [
                {
                    "scheduler": lr_scheduler.LambdaLR(
                        optimizer[0], lr_lambda=self.scheduler.schedule
                    ),
                    "interval": "step",
                    "frequency": 1,
                }
            ]

        return optimizer, scheduler

    def log_images(self, batch: Tuple[Any, Any], *args, **kwargs) -> Dict:
        log = dict()

        conds = self.get_input(batch, self.cond_key).to(self.device)
        cond_codes = self.cond_model.encode_codes(conds).detach()

        log["conditions"] = self.cond_model.to_img(conds)
        log["first samples"] = self.sample(cond_codes)
        log["second samples"] = self.sample(cond_codes)

        return log
